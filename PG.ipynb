{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "File='SRA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.95,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # http://0.0.0.0:6006/\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name=\"observations\")\n",
    "            self.tf_acts = tf.placeholder(tf.int32, [None, ], name=\"actions_num\")\n",
    "            self.tf_vt = tf.placeholder(tf.float32, [None, ], name=\"actions_value\")\n",
    "        # fc1\n",
    "        layer = tf.layers.dense(\n",
    "            inputs=self.tf_obs,\n",
    "            units=10,\n",
    "            activation=tf.nn.tanh,  # tanh activation\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "        # fc2\n",
    "        all_act = tf.layers.dense(\n",
    "            inputs=layer,\n",
    "            units=self.n_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')  # use softmax to convert to probability\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # to maximize total reward (log_p * R) is to minimize -(log_p * R), and the tf only have minimize(loss)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)   # this is negative log of chosen action\n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)  # reward guided loss\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "    #def choose_action(self, observation):\n",
    "        #prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})\n",
    "        #action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        #return action\n",
    "\n",
    "    #def store_transition(self, s, a, r):\n",
    "        #self.ep_obs.append(s)\n",
    "        #self.ep_as.append(a)\n",
    "        #self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self, s, a, r):\n",
    "        # discount and normalize episode reward\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        # train on episode\n",
    "        self.sess.run(self.train_op, feed_dict={\n",
    "             self.tf_obs: np.vstack(s),  # shape=[None, n_obs]\n",
    "             self.tf_acts: np.array(a),  # shape=[None, ]\n",
    "             self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]\n",
    "        })\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    # empty episode data\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "    def _discount_and_norm_rewards(self,r):\n",
    "        # discount episode rewards\n",
    "        discounted_ep_rs = np.zeros_like(self.r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(r))):\n",
    "            running_add = running_add * self.gamma + self.r[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        # normalize episode rewards\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Memory  ####################\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self): \n",
    "        # 一个S数组，一个R数组，一个A数组\n",
    " #dims=s.dim+a.dim+r.dim+s_.dim\n",
    "        S=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        R=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        A=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        S_=S\n",
    "        S_=S\n",
    "        \n",
    "        self.capacity = len(S)\n",
    "        self.state_dim=len(S[0])\n",
    "        self.action_dim=len(A[0])\n",
    "        \n",
    "        S_[:len(S)-1]=S[1:]\n",
    "        \n",
    "        self.data = np.hstack((S, A, R, S_))\n",
    "\n",
    "    def sample(self, n):\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'SRA/S.seqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5d721d506d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# all placeholder for tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5d2ed9fb32c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# 一个S数组，一个R数组，一个A数组\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0;31m#dims=s.dim+a.dim+r.dim+s_.dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/S'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.seqs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/S'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.seqs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/S'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.seqs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'SRA/S.seqs'"
     ]
    }
   ],
   "source": [
    "M = Memory()\n",
    "state_dim=M.state_dim\n",
    "action_dim=M.action_dim\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('A_'):\n",
    "    A = tf.placeholder(tf.float32, shape=[None, action_dim], name='a')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cb697914ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m RL = PolicyGradient(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreward_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "RL = PolicyGradient(\n",
    "    n_actions=2\n",
    "    n_features=32\n",
    "    learning_rate=0.02,\n",
    "    reward_decay=0.99,\n",
    "    # output_graph=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3000):\n",
    "\n",
    "        var *= .9995    # decay the action randomness\n",
    "        b_M = M.sample(BATCH_SIZE)\n",
    "        b_s = b_M[:, :state_dim]\n",
    "        b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "        b_a = np.clip(np.random.normal(b_a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "\n",
    "        vt =RL.learn(b_s,b_a,b_r)\n",
    "        print i_episode,vt\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
