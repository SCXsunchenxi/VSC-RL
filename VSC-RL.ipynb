{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_EP_STEPS = 200\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500) \n",
    "][0]            # you can try different target replacement strategies\n",
    "\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "HF_ACTIONBOUND=400\n",
    "SEP_ACTIONBOUND=40\n",
    "\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = False\n",
    "\n",
    "File='SRA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  Actor  ####################################\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, replacement):\n",
    "        self.sess = sess\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.replacement = replacement\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_,  scope='target_net', trainable=False)\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replace = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replace = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                 for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.3)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            net = tf.layers.dense(s, 30, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          bias_initializer=init_b, name='a', trainable=trainable)\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):   # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replace)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_a'] == 0:\n",
    "                self.sess.run(self.hard_replace)\n",
    "            self.t_replace_counter += 1\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        return self.sess.run(self.a, feed_dict={S: s})\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            # ys = policy;\n",
    "            # xs = policy's parameters;\n",
    "            # a_grads = the gradients of the policy to get more Q\n",
    "            # tf.gradients will calculate dys/dxs with a initial gradients for ys, so this is dq/da * da/dparams\n",
    "            \n",
    "            # supvised gradient\n",
    "            supvised_grads=tf.gradients(ys=tf.squared_difference(self.a, A),xs=self.e_params)\n",
    "            \n",
    "            self.policy_grads = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)+supvised_grads\n",
    "            \n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.AdamOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads, self.e_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  Critic  ####################################\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, replacement, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.replacement = replacement\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            \n",
    "            self.a = tf.stop_gradient(a)    # stop critic update flows to actor\n",
    "            # Input (s, a^), output q\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "            \n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "              \n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_q, self.q))\n",
    "            \n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "        #with tf.variable_scope('a_grad'):\n",
    "           #self.q_a=self.sess.run(self.q, feed_dict={S: S, A: a}) \n",
    "           #self.a_grads = tf.gradients(self.q_a, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replacement = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replacement = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                     for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.1)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 30\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "        \n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        _,loss=self.sess.run([self.train_op,self.loss], feed_dict={S: s, self.a: a, R: r, S_: s_})\n",
    "            \n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replacement)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_c'] == 0:\n",
    "                self.sess.run(self.hard_replacement)\n",
    "            self.t_replace_counter += 1\n",
    "        return loss\n",
    "    \n",
    "    def get_q(self,a,s):\n",
    "        return self.sess.run(self.q, feed_dict={S: s, self.a: a})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Memory  ####################\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self): \n",
    "        # 一个S数组，一个R数组，一个A数组\n",
    " #dims=s.dim+a.dim+r.dim+s_.dim\n",
    "        S=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        R=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        A=pickle.load(open(File + '/S' + '.seqs','rb'))\n",
    "        S_=S\n",
    "        S_=S\n",
    "        \n",
    "        self.capacity = len(S)\n",
    "        self.state_dim=len(S[0])\n",
    "        self.action_dim=len(A[0])\n",
    "        \n",
    "        S_[:len(S)-1]=S[1:]\n",
    "        \n",
    "        self.data = np.hstack((S, A, R, S_))\n",
    "\n",
    "    def sample(self, n):\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Memory()\n",
    "state_dim=M.state_dim\n",
    "action_dim=M.action_dim\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('A_'):\n",
    "    A = tf.placeholder(tf.float32, shape=[None, action_dim], name='a')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-b81a8efdafbe>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Create actor and critic.\n",
    "# They are actually connected to each other, details can be seen in tensorboard or in this picture:\n",
    "actor = Actor(sess, action_dim, SEP_ACTIONBOUND, LR_A, REPLACEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(sess, state_dim, action_dim, LR_C, GAMMA, REPLACEMENT,actor.a,actor.a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.add_grad_to_graph(critic.a_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "var = 3  # control exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Episode:', 0, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([38.68952], dtype=float32))\n",
      "('Episode:', 1, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([39.50118], dtype=float32))\n",
      "('Episode:', 2, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.163162], dtype=float32))\n",
      "('Episode:', 3, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.522728], dtype=float32))\n",
      "('Episode:', 4, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([39.86371], dtype=float32))\n",
      "('Episode:', 5, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.291534], dtype=float32))\n",
      "('Episode:', 6, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.272465], dtype=float32))\n",
      "('Episode:', 7, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([38.66093], dtype=float32))\n",
      "('Episode:', 8, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([37.973587], dtype=float32))\n",
      "('Episode:', 9, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([40.055866], dtype=float32))\n",
      "('Episode:', 10, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([38.854954], dtype=float32))\n",
      "('Episode:', 11, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([39.849102], dtype=float32))\n",
      "('Episode:', 12, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.059917], dtype=float32))\n",
      "('Episode:', 13, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([39.216873], dtype=float32))\n",
      "('Episode:', 14, ' Critic loss: 0.17', 'Var: 0.00', 'everage Q: %.2f', array([41.313877], dtype=float32))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-576e167cca8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_s_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_s_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mLOSS\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8fdd1bec27d8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, a, r, s_)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soft'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "for n in range(MAX_EPISODES):\n",
    "    LOSS=0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "\n",
    "        var *= .9995    # decay the action randomness\n",
    "        b_M = M.sample(BATCH_SIZE)\n",
    "        b_s = b_M[:, :state_dim]\n",
    "        b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "        b_a = np.clip(np.random.normal(b_a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "        b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "        loss=critic.learn(b_s, b_a, b_r, b_s_)\n",
    "        a = actor.choose_action(b_s)\n",
    "        critic.learn(b_s, a, b_r, b_s_)\n",
    "        actor.learn(b_s)\n",
    "        LOSS+=loss\n",
    "        \n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            LOSS=LOSS/MAX_EP_STEPS\n",
    "            \n",
    "            \n",
    "    \n",
    "    #对测试集\n",
    "    b_M = M.sample(BATCH_SIZE)\n",
    "    b_s = b_M[:, :state_dim]\n",
    "    b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "    b_a = np.clip(np.random.normal(b_a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "    b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "    b_s_ = b_M[:, -state_dim:]\n",
    "    a = actor.choose_action(b_s)\n",
    "    q_a = critic.get_q(a,b_s)\n",
    "    eveQ=0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        eveQ+=q_a[i]\n",
    "    eveQ=eveQ/BATCH_SIZE\n",
    "    \n",
    "    print('Episode:', n, ' Critic loss: %.2f' % LOSS, 'Var: %.2f' % var,'everage Q: %.2f',eveQ)\n",
    "    \n",
    "print('Running time: ', time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
